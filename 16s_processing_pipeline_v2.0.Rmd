---
title: "16s Processing Methods"
author: "Katie McCauley (kathryn.mccauley@ucsf.edu)"
date: "Last Modified: October 2022 (v2.1)"
output:
  html_document: 
    toc: yes
    toc_depth: 4
    theme: lumen
    highlight: zenburn
---

```{r, setup, include=FALSE}
knitr::opts_knit$set(root.dir = 'assets/')
```

# From Sequencer to ASV Table

After your sequencing run is complete, there are a few methods to obtain the sequencing data in its raw format for downstream analysis. (1) Download from basespace when the run has been linked, or (2) download directly from an external drive that may or may not be connected to the Illumina machine.


## A. Download from BaseSpace

In order to download from BaseSpace, the first thing you will need to do is download the Command Line Interface (CLI) for BaseSpace following the directions here `https://developer.basespace.illumina.com/docs/content/documentation/cli/cli-overview` using the Linux method. You will want to install onto Wynton in your home (`~`) directory. Specifically run the following commands on Wynton:

```
cd ~
wget "https://api.bintray.com/content/basespace/BaseSpaceCLI-EarlyAccess-BIN/latest/\$latest/amd64-linux/bs?bt_package=latest" -O $HOME/bin/bs
chmod u+x $HOME/bin/bs
~/bin/bs auth
```

The output will be a link that you copy into your browser, and you will sign into basespace, thereby linking the `bs` command to your account. You will only need to do this once. Once complete, return to the command line.

There are several options available once you have BaseSpace installed. Review the link above to find the commands you need. Commands like `~/bin/bs list run` will list out all of the runs that are available to you, including the project identifier, which will be helpful when downloading (`~/bin/bs download run --id {id} --output {run_name}`). Please `cp` your workspace to the `/wynton/group/lynch/NextSeq_data/` directory before running the download command so that the run downloads to our raw run directory.

### B. Download From External Drive/Sequencer

If you completed the steps in A and find that within the run directory you don't have a directory called `Data`, you may need to download directly from the external drive connected to the sequencer. If connected, the data should be there already, so you can simply walk the drive to your computer.

Now, you will upload the data to Wynton where we will do all sequence data processing.

Start by plugging your drive into your computer. If you have a Mac, there are two Terminal commands that can upload data: `scp` and `rsync`. There are benefits to both, but for the purpose of uploading sequence data, scp is better because it gives informative progress text. An example usage for your needs would be:

```
scp -r ~/Volumes/MyDrive/190826_NS500170_0076_AH772CBGXC/ {username}@dt2.wynton.ucsf.edu:/wynton/group/lynch/NextSeq_data/
```

A couple of things to note:

- The -r option means that it will upload all of the files within the run directory.

- Here we are uploading using the data transfer node on Wynton (hence, dt2), which offers much faster upload speeds than using a login node. We are also uploading to a directory on Wynton where our raw runs live. This is important for downstream processing.

If you are running on a Windows machine, you will have more success with software like FileZilla and using an SFTP connection (port 22!). Speak with Katie if you need support with this.

You will now need to upload the mapping file using the same method as above, and it will need to be saved in `/wynton/group/lynch/NextSeq_Processed/mapping_files`. This will then set you up perfectly for processing the data using the developed script.

# Pipeline #1 (QIIME mapping file)

We originally built our sequencing data pipelines to utilize QIIME mapping files. We now use the Illumina-based mapping files for most new runs, but the QIIME mapping file pipeline can be helpful for "legacy" sequencing runs without needing to fully re-build your mapping file. This file has a .txt extension and typically starts with a header of column names that include `#SampleID` first and `Description` at the end.

## Required Rules for QIIME-based Mapping Files

Mapping files need to be in a *very* specific format for the software we use to be able to read them in correctly. You can find all of the details [here](http://qiime.org/documentation/file_formats.html) or [here](https://gls9000.gitbooks.io/qiime-tutorial-for-illumina-paired-end-sequences/content/1_preparing_the_data/14_preparing_a_mapping_file.html). The long-and-short of it is that your mapping file may not pass QC for a number of reasons (one of the main reasons the whole pipeline breaks within the first 10 minutes). The rules are:

1. The file should be named with the same date as the run (so `Nextseq_YYMMDD_mapping.txt` matching `YYMMDD_Nextseq500_...`). PLEASE DO THIS. It matches a run to a mapping file. Three years down the road, we won't care that you made your mapping file two days after you started the run, we just want to know what run it matches to, and the date is the most interpretable and memorable marker of that.

2. Should be a tab-delimited file (txt)

3. The first variable needs to be "#SampleID" (with the hashtag/pound sign, and all applicable capitalization)

4. Second and third columns should be BarcodeSequence and LinkerPrimerSequence, in that order, though LinkerPrimerSequence contents don't matter (I believe). Barcode sequence *definitely* matters.

5. No duplicate column names

6. NO BLANK VALUES -- ANYWHERE. The new tracking sheet highlights unfilled values in red. Please check that page for any blank values and fill them in before downloading the mapping file as txt.

7. With QIIME, while periods (.) are okay, you can't use underscores (_), dashes(-), or **any other special characters** in the entire file.

8. We have also discussed adding other descriptive columns to the mapping file including final DNA concentration or 16S copy number, just as additional QC checks if we need to make sure a sample's composition is consistent with other measured factors from processing

9. The last column needs to be called "Description" and be filled with the sample name again

10. All negative controls and empty wells should use "NTC" and "EMTPY" respectively in the **SampleType** variable. This is the variable used in the Negative Control filtering step to identify negative controls in the dataset. If the script breaks at the negative control filtering step, this is a very likely culprit.

An example of an older mapping file that works with the script, with sample names changed to generic. Clearly you want to make sure that the sample names match up with the data you're about to analyze. This dummy file can be downloaded from the [Lynch Lab Pipeline website](https://lynchlab-ucsf.github.io).

```{r, echo=FALSE, results='asis', message=FALSE}
library(knitr)
library(kableExtra)
library(dplyr)
map <- read.table("Nextseq_YYMMDD_mapping.txt", header=TRUE, check.names=F, sep="\t", comment="")
map %>% 
  kable(format = "html",escape=F) %>% 
  kable_styling("striped", full_width=F) %>%
  add_header_above(c("Table 3"=nrow(.))) %>% 
  scroll_box(width = "120%", height = "5in")
```

## Processing a Raw NextSeq Run into an ASV table (QIIME)

Before you begin, if you have a SampleSheet.csv file in the raw run files, delete it! Katie has encountered unexpected bugs in the past due to this file.

The steps needed to process a NextSeq run and develop an ASV table have already been pipelined. Assuming the runs have been set up as above (run directory in /NextSeq_data/ and mapping file in /NextSeq_Processed/mapping_file/ with the modified formatting), you are ready to get started. Katie has recently upgraded the pipeline to utilize a Docker container so there is no need to install any packages to run the pipeline now. You can literally just push "go!"!

Once complete, start processing the run:

```
cd /wynton/group/lynch/NextSeq_Processed/
qsub -M {your-email} scripts/qiime_mapping_file_pipeline.sh 190826_NS500170_0076_AH772CBGXC Nextseq_190826_mapping.txt
```

The -M option will send you an e-mail when the process finishes. `scripts/complete_16s_pipeline.sh` is the name of the bash file with all of the commands. `190826_NS500170_0076_AH772CBGXC` is the run name. `Nextseq_190826_mapping.txt` is the mapping file (which should live in the mapping_files directory). After several hours (16-24), you will have the following directories in the directory for this run within NextSeq_Processed:

- `submission`: This directory contains all of your R1 and R2 files separated by sample ID. This is what gets uploaded to SRA and also what is used for DADA2.

- `FASTQC` contains the results from FastQC You will need to download the files onto your computer to be able to view them. Katie has also implemented MultiQC which gives an overview of the FastQC results as one output file instead of a large set of FastQC files and going through them individually.

- `dada2_output` contains all of the files generated from DADA2. These include:
  - TrackedReadsThruDADA2.csv
  - TaxonomyBootstraps.csv
  - {runname}_seqs.txt (Just the ASVs)
  - {runname}_tax_table.txt (Taxonomy Table from DADA2)
  - {runname}_tax_table_DECIPHER.txt (Taxonomy Table from DECIPHER)
  - {runname}_otutable.txt (ASV Table of counts)
  - dada2_result_object.RData (Final dada2 object -- if you need to troubleshoot something, you can load this object back into your environment)
  - dada2_phy_obj_raw.rds (Raw phyloseq object from the DADA2 script)
  - dada2_optim_tree.tre (Tree built from sequences)
  - dada2_phy_pruned_wtree.rds (Phyloseq object with low-prevalence filtering and a tree object)
  - Combined_Cleaning_Figure.pdf (Figure generated before and after negative control filtering -- definitely review this figure to confirm that the defaults worked for your data)
  - phyloseq_noneg.rds (Final phyloseq object with negative control signal removed as per the figure)

The file that you will use for subsequent analysis is `phyloseq_noneg.rds`. If you don't get some of the later files, you may need to start with `dada2_phy_obj_raw.rds` and work through the interim steps separately.

### Steps in the Script

#### bcl2fastq

Illumina's proprietary hyper-compressed files are called BCL (or base call) files. They can be easily transformed into usable FastQ (Q because the quality information is included) files using this command. We're not providing a mapping file, so all obtained reads are placed into "Undetermined" files, which means that the script wasn't able to associate a barcode with a sample name. For this pipeline, this is fine and expected.

#### Undetermined FastQC

Next, we perform FastQC on the large undetermined files, which means that we are getting an overall quality profile of the reads. This gives us an additional sense if anything went awry with the run.

#### Split Libraries

"Splitting libraries" is essentially the practice of assigning a sample name to a read, and this is done with your well-formatted mapping file via some QIIME scripts. It matches the barcode from the FastQ file of barcodes to the list of barcodes in your mapping file and creates a new "pool" of sequences that have been assigned to a sample.

#### Poly-G Filter

The NextSeq in particular is on a two-color system. That means that by using just red and green colors, it can relay information for all four nucleotides. How, you ask? Through permutation! In other words, only green and only red are two nucleotides, but you get the other two by doing both and neither, respectively! How cool!!! Except.... Gs are created when you get no light, and sometimes you can get no light for reasons beyond a G nucleotide, creating long stretches of Gs in your sequence data. This makes DADA2 unhappy, so I use bbduk to filter out these "poly G" reads using an entropy (or sequence similarity score) of 0.2.

#### Make Sample-Specific FASTQs

Pretty straight-forward. Just makes one fastq file for each of the forward and reverse reads for each sample, which is the input to DADA2's algorithm.

# Pipeline #2 (Illumina Sample Sheet/Version 2)

As of recently, a pipeline based on the formatting of the Illumina Sample Sheet has been developed. This is likely the script that you will be using most-frequently unless you are working with a "legacy" run. The components of this pipeline are essentially the same as described above with a few behind-the-scenes modifications that allow the use of Illumina's bcl2fastq software to actually perform the "splitting libraries" step instead of having QIIME do it (and raising reviewer eyes when trying to write methods).

## Rules for the Illumina Sample Sheet

The rules associated with the Illumina Sample Sheet are annoyingly different, but relatively simple compared to the QIIME sample sheet. Specifically:

1. Sample names can only contain '-' and '_' as non-alphanumeric characters. Which means no periods, slashes, etc.

2. Again, no empty values anywhere in the dataset

3. The Illumina Sample Sheet should be saved as a CSV (and not Excel's "top" CSV format with UTF-8 encoding, but the one lower down: `Comma Separated Values (.csv)`). This is annoying but vital.

4. Sample sheets should be named similarly to how we've named them in QIIME format, except that the extension will be .csv instead of .txt. So, for example, the Illumina sheet should be named: `Nextseq_YYMMDD_mapping.csv`

## Using the new script

After uploading the mapping file to `/wynton/group/lynch/NextSeq_Processed/mapping_files/`, log in to Wynton. Again, all software and R packages you need to run the pipeline are now "pre-installed" via a Docker container, so you can simply initiate the script by running: 

```
cd /wynton/group/lynch/NextSeq_Processed/
qsub scripts/illumina_sample_sheet_pipeline.sh 190826_NS500170_0076_AH772CBGXC Nextseq_190826_mapping.csv
```

# Post-Processing Steps

Low-prevalence filtering and negative control signal filtering are both part of our "usual" processing and filtering methods for most datasets and are completed automatically as part of the script above. Multiply-rarefying, however, is a process that requires a few additional steps to identify the optimal depth, so it needs to be done manually.

### 1. Low-prevalence filtering

Next, we want to remove any SVs in our table that may be contaminants, or generally present in extremely low counts relative to our total count, cutting down on noise in our data. This is typically 0.0001% of the total read count in your dataset.

### 2. Negative control filtering

Next, we use a script to remove signal deriving from the negative controls from our samples. This form of negative control filtering outright removes any SVs found in more than 15% of your negative controls and less than 15% of samples. Among the SVs that remaine, the average read count within negative controls is subtracted from the read count in samples, and negative numbers are returned to 0.

The script then returns a NTC-filtered phyloseq object. Also output by the script is a figure showing the distribution of taxa before and after running the script (Combined_Cleaning_Figure.pdf). An example output is shown here (this plot coming from running the same example SV table through the NTC script):

![Image](assets/Combined_Cleaning_Figure.png)

We can tell from this figure that taxa like **Pseudomonas** and **Delftia** were removed, and while some taxa also present in negative controls remain, they are very likely to be biologically meaningful. Further examination of this data (not shown) found that remaining taxa had a very low total read count within the negative controls. Hence, the lack of movement of those taxa.

# Normalizing for Differences in Read Depth

All microbiome data needs to be normalized for differences in read depth. This can be done through either Multiply Rarefying or Variance Stabilized Transformations.

Considerations for your own dataset should be driven by the questions you wish to answer. Our preference is for multiply-rarefying, but if you have low-abundance samples that you cannot filter out you may want to consider variance-stabilized transformations.

### A. Rarefying

Rarefying means to sub-sample each sample to an even read depth. We use a method for *representative* rarefaction in which we sub-sample each sample 100 times and choose the randomized sample at the center of a distance matrix. However, before we can carry out this process, we need to determine the optimal depth. This depth is that at which there is no additional diversity to gain and you retain an optimal number of samples (samples with a read depth lower than the rarefying depth are removed).

I have built a script to carry out this process. In order to use it, you can run:

```
Rscript /wynton/group/lynch/kmccauley/dada2_files/alpha_rarefaction.R phyloseq_noneg.rds transpose
```

The `transpose` at the end of the script indicates that the table of counts should be transposed before generating rarefaction curves. If you wind up with a figure that shows rarefaction curves for sequence names, change the `transpose` to another value (all that matters is that it's *not* `transpose` but is still one word).

This script runs slowly. Once it's done, you will get two outputs from this command (1) a table of read depths and the number of samples remaining at that read depth and (2) a figure representing the curves for each sample. In order to view a figure made on Wynton, you can copy it to your desktop using either your favorite FTP or the following scp command from your computer:

```
scp {username}@dt2.wynton.ucsf.edu:{image_location}/Alpha_Rarefaction_Curve.pdf ~/Downloads/
```

![Image](assets/AlphaRarefactionPlot.pdf)

From this figure and the associated table printed in your console, you want to identify the depth where diversity plateaus, but you don't end up losing many samples. In this example, we would likely pick something in the 50,000-70,000 read range, since diversity has plateaued and we start losing samples due to read depth soon after.

To initiate the multiply-rarefying step, you can run the following script from the location of your phyloseq object on Wynton. Before this point, you likely want to copy the `phyloseq_noneg.rds` file into your own analysis directory, perhaps giving it a more illustrative name.

```
Rscript /wynton/group/lynch/kmccauley/dada2_files/Multiply_Rarefy_phy.R phyloseq_noneg.rds 50000 phyloseq_mrare.rds
```

The first object is the name/location of the script. `phyloseq_noneg.rds` indicates the name of the input phyloseq object; `50000` is the planned read depth; `phyloseq_mrare.rds` is the name of the output object. If you'd rather not specify an output name, one will be chosen for you.

#### Procrustes

Let's say you're curious to know if the community composition of a dataset rarefied down to 50,000 reads is more or less informative than one rarefied to 70,000 reads. You can rarefy at both depths and then use a procrustes test to determine if their structure is significantly different, and I have developed a script to test this. To do this, you can run something like the following:

```
## Rarefy to 50k reads
Rscript /wynton/group/lynch/kmccauley/dada2_files/Multiply_Rarefy_phy.R phyloseq_noneg.rds 50000 phyloseq_mrare_50k.rds
## Rarefy to 70k reads
Rscript /wynton/group/lynch/kmccauley/dada2_files/Multiply_Rarefy_phy.R phyloseq_noneg.rds 70000 phyloseq_mrare_70k.rds
## Compare with Procrustes
Rscript /wynton/group/lynch/kmccauley/dada2_files/procrustes.R phyloseq_mrare_50k.rds phyloseq_mrare_70k.rds
```

The fun thing about the output is that if there is a significant P-value, it means that there is not a significant difference between the two datasets. The default distance matrix is currently Bray Curtis, but you can specify your preferred distance matrix by using one of the phyloseq distance matrix names at the end of the function (`bray`, `canberra`, `unifrac`, `wunifrac`).

### B. Variance Stabilized Tranformations

We can also use the variance stabilized transformation, which can be applied before downstream analyses. Despite allowing for small read depths, you'll still want to consider removing samples with less than 1000 reads since those profiles may not be as robust. However, if you are working with ultra-low-burden samples you may not be able to do that either. As we start getting into some of these circumstances, a chat with Katie or Sue may be helpful in determining how best to move forward given your study.

```
library(DESeq2)
library(phyloseq)
## read in the saved RDS file:
phy <- readRDS("phyloseq_noneg.rds")
phy.filt <- subset_samples(phy, sample_sums(phy) > 1000) ## filter to samples with greater than 1000 reads
phy.vst <- phy.filt ## Initialize a new object where we'll save the VST object / keeps us from overwriting the original data.
deseqdat <- phyloseq_to_deseq2(phy.vst, ~1) ## We can use this ~1 as our "design formula" since we're looking to normalize the data.
deseqdat2 = estimateSizeFactors(deseqdat, type="poscounts")
otu_table(phy.vst) <- otu_table(counts(deseqdat2, normalized=TRUE),taxa_are_rows=TRUE) ## Update OTU table in the VST object we saved.
```

At this point, you're now ready to start the fun part! Analyzing and visualizing your data!

# Acknowlegements

Katie would like to give many thanks to Danny Li who helped develop the initial version of the analysis pipeline, as well as the several members of the Lynch Lab and BCMM who provided invaluable constructive feedback along the way.